\section{Odhad parametrov}
Podobne dôležitá ako samotná štruktúra resp. voľba dátového modelu, je aj odhad jeho parametrov. V tejto časti spomenieme niektoré z najviac využívaných metód na identifikáciu či už statických alebo dynamických modelov. 

Skôr ako začneme rozoberať samotné metódy, je potrebné spomenúť základné požiadavky na odhad parametrov~\cite{beck:param_est:1977}, ktoré by mala každá dobrá metóda spĺňať. Tieto požiadavky tvoria súhrn štatistických vlastností, ktoré chceme, aby daná metóda mala, pretože vedú k správnemu, možno by bolo lepšie povedať k optimálnemu, odhadu parametrov.

\subsection{Požiadavky na odhad parametrov}
Majme vektor $ \hat{\theta} $, ktorý predstavuje vektor odhadnutých parametrov $ \theta $, potom:
\begin{itemize}
	\item[] \textbf{nevychýlenosť} odhadu definujeme ako
	\begin{equation*}
		E\left\lbrace \hat{\theta}^k \right\rbrace = \theta, 
	\end{equation*}
	čo znamená, že ak odhad parametrov uskutočníme na základe $ k $ meraní, potom pre každé meranie $ k $ musí platiť, že stredná hodnota odhadnutých parametrov $ E\left\lbrace \hat{\theta}^k \right\rbrace $ sa rovná ich skutočným hodnotám $ \theta $.
	\item[] \textbf{konzistencia} znamená, že pre ľubovoľný vektor konštánt $ \delta > 0 $ platí
	\begin{equation*}
		\lim\limits_{k \rightarrow \infty} P \left( \left| \hat{\theta}^k - \theta \right| < \delta \right) = 1. 
	\end{equation*}
	Táto definícia v podstate tvrdí, že so zväčšujúcim sa počtom dát, by sa odhad parametrov mal zlepšovať resp. by sa mal blížiť ku skutočným hodnotám (pravdepodobnosť, že absolútna hodnota rozdielu odhadovaných a skutočných parametrov $ P \left( \left| \hat{\theta}^k - \theta \right| \right) $ bude ležať v ľubovoľnom $ \delta $ okolí, bude mať hodnotu istého javu).
	\item[] \textbf{výdatnosť}, t.j., ak medzi odhadom $ \hat{\theta} $ a ľubovoľným ďalším odhadom $ \tilde{\theta} $ platí 
	\begin{equation*}
		\text{Cov} \left( \tilde{\theta} \right) - \text{Cov} \left( \hat{\theta} \right) \geq 0,
	\end{equation*}
	teda optimálny odhad je ten, ktorý má minimálnu disperziu resp. kovarianciu $ \text{Cov} \left(\right) $ (v skalárnom prípade). Pri nulovej disperzii prechádza pravdepodobnosť v istotu. Výdatnosť sa v literatúre môže označovať taktiež ako efektívnosť \cite{beck:param_est:1977}.
\end{itemize}

\subsection{Metódy statickej identifikácie}
\subsubsection*{Metóda najmenších štvorcov}
Jednou z najpoužívanejších metód, ktorú vynašiel Gauss pri výpočte obežných dráh komét a planét z nameraných údajov, je metóda najmenších štvorcov \cite{hostetter:recursive_est:1987}. Princíp tejto metódy je založený na hľadaní takej kombinácie parametrov systému, ktorá minimalizuje vzdialenosť medzi nameranými a odhadnutými údajmi. Treba zdôrazniť, že táto metóda vyžaduje, aby parametre modelu boli v lineárnom vzťahu k vstupom.

Začnime nasledovne. Majme systém $ F(\theta, x) $, ktorý je funkciou vstupov modelu $ x $ a parametrov $ \theta $. Výstup $ y $ z takéhoto systému, ktorý je zaťažený chybou merania $ e $, je
\begin{equation}
	y = \theta^T x + e = \theta_1x_1 + \theta_2x_2 + \dots + \theta_kx_k.
\end{equation}
Pokúsme sa nájsť, takú hodnotu parametrov $ \hat{\theta} $, ktorá by minimalizovala súčet druhých mocnín odchýliek nameraných údajov od modelových výstupov ako
\begin{equation}
	J\left(\theta\right) = \sum_{i=1}^{k} e_i^2 = \sum_{i=1}^{k} \left(y_i - \theta^T x_i\right)^2.
\end{equation}
Túto rovnicu môžeme napísať vo vektorovom tvare
\begin{equation}
	J\left(\theta\right) = \left(Y - X\theta \right)^T \left(Y - X\theta \right).
\end{equation} 
Formálne sa rovnica nezmenila, iba sme zadefinovali dva nové vektory
\begin{equation}
	Y = \begin{pmatrix}
			y_1 \\
			y_2 \\
			\vdots \\
			y_k
		\end{pmatrix}, \qquad
	X = \begin{pmatrix}
			x_1^T \\
			x_2^T \\
			\vdots \\
			x_k^T
		\end{pmatrix}.
\end{equation}
Podmienku minima zabezpečíme z nulovej hodnoty gradientu funkcie $ J(\theta) $ podľa $ \theta $.
\begin{equation}
	\nabla J \left(\theta\right) = \frac{\partial J \left(\theta\right)}{\partial \theta} = -X^T Y + X^T X\hat{\theta} = 0.
\end{equation}
Z toho jasne vyplýva, že ak existuje inverzia výrazu $ X^T X $, vektor odhadovaných parametrov získame ako 
\begin{equation}
	\hat{\theta} = \left(X^T X\right)^{-1}X^T Y.
\end{equation}
Bez dôkazu uvádzame, že metóda najmenších štvorcov spĺňa všetky požiadavky na odhad parametrov \cite{fikar:identifikacia:1999}.

\subsubsection*{Modifikovaná metóda najmenších štvorcov}
V prípade, že náhodný šum $ e $ je korelovaný a poznáme jeho kovariančnú maticu $ \Sigma $, môžeme prepísať kritérium minimalizácie do tvaru 
\begin{equation}
	J\left(\theta\right) = \left(Y - X\theta \right)^T \Sigma^{-1} \left(Y - X\theta \right).
\end{equation}
Ak uvažujeme stochastický proces, potom najlepší nevychýlený odhad parametrov $ \hat{\theta} $ získame ako
\begin{equation}
	\hat{\theta} = \left(X^T \Sigma^{-1} X\right)^{-1}X^T \Sigma^{-1} Y.
\end{equation}
Takúto modifikáciu metódy najmenších štvorcov používame najmä vtedy, ak sa v meraní vyskytujú systematické chyby, zaznamenávanie nameraných údajov má časové oneskorenie, disponujeme nesprávnym modelom, zvolili sme nesprávne vstupné veličiny alebo namerané dáta boli nejakým spôsobom filtrované alebo extrapolované \cite{fikar:identifikacia:1999}.

\subsubsection*{Formulácia vhodnej optimalizačnej úlohy}
Metóda najmenších štvorcov, napriek tomu ako veľmi elegantne si dokáže poradiť s odhadom parametrov, má jednú veľkú nevýhodu. Parametre modelu musia byť v lineárnom vzťahu ku vstupom, pričom samotné vstupy môžu predstavovať ľubovoľné lineárne aj nelineárne funkcie. Týmto sa eliminuje značná časť problematík, ktoré jednoducho nedokáže vyriešiť. 

Ukazuje sa, že vhodným definovaním optimalizačného problému, dokážeme spomenutý problém obísť a nie len to. Takýto prístup nám umožňuje odhadovať parametre dynamických systémov, teda diferenciálnych alebo diferenčných rovníc \cite{villaverde:opt_param_est:2018}.

Majme ľubovoľnú diferenciálnu rovnicu
\begin{equation}
	\dot{x}(t) = f\left( t, x(t), \theta \right),
\end{equation} 
a súbor nameraných údajov $ y = [y_1, y_2, \dots , y_N] $ z daného dynamického systému. Budeme hľadať takú kombináciu parametrov $ \hat{\theta} $, ktorá minimalizuje sumu kvadrátu rozdielu nameraných a modelových údajov. Odhad parametrov potom získame ako
\begin{equation}
	\begin{split}
		\hat{\theta} = \arg \min_{\theta} \quad & \sum_{i=1}^{N} \left(y_{i} - x(t_{i})\right)^2. \\
		\textrm{s.t.} \quad & \dot{x}(t) = f\left(t,x(t),\theta \right)\\
		 & x(0) = x_0
	\end{split}
	\label{eq:param_est_opt_form}
\end{equation} 
Ako si môžeme všimnúť, účelová funkcia takto zadefinovanej optimalizačnej úlohy a metódy najmenších štvorcov je totožná. Rozdiel je v tom, že zatiaľ čo pri metóde najmenších štvorcov sme boli schopný nájsť analytické riešenie, v tomto prípade je nutné riešiť danú optimalizačnú úlohu numericky a takýto prístup k odhadu parametrov je výpočtovo náročnejší.

